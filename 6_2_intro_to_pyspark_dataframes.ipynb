{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vafter341ew/COLAB/blob/main/6_2_intro_to_pyspark_dataframes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9tijlvsDX3B"
      },
      "source": [
        "#### Colab Prep\n",
        "\n",
        "Execute the following code cells to whenever you open/restart the notebook in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-aDkksMDX3B"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/WSU-DataScience/dsci_325_module6_basic_data_management_in_python/raw/main/sample_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHhvZrS2DX3B",
        "outputId": "a8869253-d0e6-45e8-98f4-7d78809404a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./sample_data.zip\n",
            "replace __MACOSX/._sample_data? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip ./sample_data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmGoJGPARy2q"
      },
      "source": [
        "# Introduction to `pyspark.sql.DataFrame`s\n",
        "\n",
        "Adapted from [Databrick's tutorial](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdkg61RKDX3C"
      },
      "source": [
        "## Installing a `pyspark` Anaconda virtual environment\n",
        "\n",
        "Use Anaconda Navigator to create a virtual environment with the following packages installed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNV0Y9M0DX3C"
      },
      "source": [
        "#### `pyspark` Stuff\n",
        "1. `openjdk` to install Java,\n",
        "2. `pyspark` to install `spark` and `pyspark`, and\n",
        "3. `findspark` to (possibly) deal with any issues finding `spark`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54Ez9ZJdDX3C"
      },
      "source": [
        "#### The usual suspects - data management\n",
        "\n",
        "1. `pandas`\n",
        "2. `polars`\n",
        "3. `pyarrow[all]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9QlAPrxDX3C"
      },
      "source": [
        "#### The usual suspects - visualization and ML\n",
        "\n",
        "1. `scikit-learn`\n",
        "2. `seaborn`\n",
        "3. `plotnine`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTDeNjoDDX3C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow\n",
        "\n",
        "pyarrow.__version__"
      ],
      "metadata": {
        "id": "ESwjTVefDioA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5K2nZoKRy2u"
      },
      "outputs": [],
      "source": [
        "# import pyspark class Row from module sql\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D24uSpsiRy2v"
      },
      "source": [
        "## What is spark?\n",
        "\n",
        "* Build for the Hadoop platform\n",
        "* Replacement of MapReduce\n",
        "* Second-generation optimization\n",
        "    * Lazy\n",
        "    * Optimized\n",
        "    * Persistent data structures\n",
        "* Written in scala"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw5GmuzoRy2v"
      },
      "source": [
        "## Ok ... so what's Hadoop?\n",
        "\n",
        "* Distributed computing platform\n",
        "* [Used by lots of companies](https://wiki.apache.org/hadoop/PoweredBy)\n",
        "* Becoming an industry standard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qkETupURy2w"
      },
      "source": [
        "## What is `pyspark`?\n",
        "\n",
        "* Python interface to spark\n",
        "* Needs a spark session\n",
        "    * `session` $\\leftrightarrow$ spark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v26KFc3Ry2w"
      },
      "source": [
        "## Step 0 - Create a spark session\n",
        "\n",
        "`pyspark` (Python) communicates with `spark` (JVM via Scala) through a session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inmnI_3_Ry2x"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName('Ops').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZct8N4PRy2y"
      },
      "source": [
        "## Overview -  `pyspark.DataFrame`\n",
        "\n",
        "* A `DataFrame` is a collection of `Row`s\n",
        "* `Row`s can be distributed over many machines\n",
        "* `spark`\n",
        "    * Hides the messy details\n",
        "    * Optimizes operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKVuBwbtRy26"
      },
      "source": [
        "## How to think about a `pyspark.DataFrame`\n",
        "\n",
        "<img src=\"https://github.com/wsu-stat489/module5_intro_to_pyspark/blob/main/img/pyspark_df.png?raw=1\" width=600>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKrVK7sARy28"
      },
      "source": [
        "## Reading a `csv` file with `spark.read.csv`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6u3wfcxDX3D"
      },
      "source": [
        "#### `read.csv` is lazy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhbWSRcWRy29"
      },
      "outputs": [],
      "source": [
        "(heroes :=\n",
        " spark.read.csv('./sample_data/heroes_information.csv', header=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLf4rjDrRy26"
      },
      "source": [
        "## Example - `filter` and `collect`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0Hf0zC7DX3D"
      },
      "source": [
        "#### `filter` is lazy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVnw28aPDX3D"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "(heroes\n",
        " .filter(col('Eye color') == 'yellow')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yfBTLzIDX3D"
      },
      "source": [
        "#### `limit` is lazy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Pc7RLFdDX3D"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "(heroes\n",
        " .filter(col('Eye color') == 'yellow')\n",
        " .limit(5)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ec0dvmHDX3D"
      },
      "source": [
        "#### `take` is eager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alBQFwpPDX3D"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "(heroes\n",
        " .filter(col('Eye color') == 'yellow')\n",
        " .take(5)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9_R25JcDX3D"
      },
      "source": [
        "#### `collect` is eager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQDPw6WPDX3D"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "(heroes\n",
        " .filter(col('Eye color') == 'yellow')\n",
        "#  .limit(5)\n",
        " .collect()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9SJPtXWRy27"
      },
      "source": [
        "### Why is `pyspark` so slow?\n",
        "\n",
        "* Optimized for\n",
        "    * Distributed computation\n",
        "    * Big data\n",
        "* Not great for\n",
        "    * Local work on\n",
        "    * Small data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNUlpuSUDX3D"
      },
      "source": [
        "### Why is `pyspark` so fast?\n",
        "\n",
        "* Distributed nature $\\longrightarrow$ leverage multi-core CPU,\n",
        "* Data model can optimize data access via predicate/projection/slice pushdown,\n",
        "* Lazy evaluation allow optimized memory usages (e.g., for a grouped aggregation), and\n",
        "* Arrow allows FAST implementation of `pandas` user defined functions (UDF).\n",
        "\n",
        "See [this article](https://www.databricks.com/blog/2018/05/03/benchmarking-apache-spark-on-a-single-node-machine.html) for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeGQ8V3WRy28"
      },
      "source": [
        "## `filter` and `collect` illustrated\n",
        "\n",
        "<img src=\"https://github.com/wsu-stat489/module5_intro_to_pyspark/blob/main/img/pyspark_filter_collect.gif?raw=1\" width=600>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeWlqKJhRy29"
      },
      "source": [
        "## Inspecting the column types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxukDxMURy2-"
      },
      "outputs": [],
      "source": [
        "heroes.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_fo_qJRRy2-"
      },
      "source": [
        "## Gathering results in `pyspark.sql`\n",
        "\n",
        "* **Important fact** All `pyspark` queries end in a collection method\n",
        "* **Why?** Data is (possibly) spread across many machines\n",
        "* <font color = \"red\"> **Warning** This might be is *expensive*! Know how much data your are requesting! </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0RzEUbPRy2_"
      },
      "source": [
        "## Gathering methods\n",
        "\n",
        "Here are the default methods for gathering the results.\n",
        "\n",
        "* `collect` returns all values\n",
        "* `take(n)` returns the first `n` values\n",
        "* `sample(n)` returns `n` randomly selected values\n",
        "\n",
        "**Note.** These are combersome, as they return a list of `Row`s :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ps4FCptRy2_"
      },
      "source": [
        "### Inspecting the content - `take`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChQBZx0DRy3A"
      },
      "outputs": [],
      "source": [
        "heroes.take(5) # BAD!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVUEEQsoRy3B"
      },
      "source": [
        "## Inspecting the whole table - `collect`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ukr-YV_dRy3C",
        "scrolled": true,
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "heroes.collect() # BAD!!!1!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oycdPLKEDX3L"
      },
      "source": [
        "## Converting to `pandas` using `pyarrow`\n",
        "\n",
        "If we have `pyarrow` installed, we can use the `toPandas` method to collect the data and convert to `pandas`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oatA7gKQDX3L"
      },
      "source": [
        "#### Use `limit` to collect the head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fLu0OS2DX3L"
      },
      "outputs": [],
      "source": [
        "heroes.limit(5).toPandas() # Good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH7MSj1JDX3L"
      },
      "source": [
        "#### Use `sample` and `toPandas` to get a random sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WZk_EFvDX3L"
      },
      "outputs": [],
      "source": [
        "(sample :=\n",
        " heroes\n",
        " .sample(fraction=0.01)\n",
        ").toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXTQbCrqDX3L"
      },
      "source": [
        "#### Use `toPandas` to collect the whole table (careful...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xooqTn9PDX3L"
      },
      "outputs": [],
      "source": [
        "heroes.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zndd6lmLRy3H"
      },
      "source": [
        "## Houston, we have a problem! (Did you notice?)\n",
        "\n",
        "<img src=\"https://github.com/wsu-stat489/module5_intro_to_pyspark/blob/main/img/pyspark_missing_values.png?raw=1\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCqrpp4aRy3I"
      },
      "source": [
        "### Specifying a `nullValue`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "043y77KwRy3J"
      },
      "outputs": [],
      "source": [
        "(heros :=\n",
        " spark.read.csv('./sample_data/heroes_information.csv', header=True, nullValue='-')\n",
        ").limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt-HmftzRy3K"
      },
      "source": [
        "### Did you notice?\n",
        "\n",
        "<img src=\"https://github.com/wsu-stat489/module5_intro_to_pyspark/blob/main/img/pyspark_default_types.png?raw=1\" width=400>\n",
        "\n",
        "Default type is a string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqF2ZmQTRy3L"
      },
      "source": [
        "### Letting `spark` guess the types\n",
        "\n",
        "Set `inferScheme=True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0n7j00LRy3L"
      },
      "outputs": [],
      "source": [
        "(heros :=\n",
        " spark.read.csv('./sample_data/heroes_information.csv', header=True, inferSchema=True, nullValue='-')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ULZOaHRy3L"
      },
      "source": [
        "## Checking the column types after `inferScheme`\n",
        "\n",
        "In this case, `spark` guessed correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_U8DQmzRy3M"
      },
      "outputs": [],
      "source": [
        "heros.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkheoz2-Ry3M"
      },
      "source": [
        "## Inspecting the content - `limit(5).toPandas()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpX44QYnRy3M"
      },
      "outputs": [],
      "source": [
        "heros.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN6uO88SRy3N"
      },
      "source": [
        "## Explicit `schema` specification\n",
        "\n",
        "Format is `add(name, type, nullable?)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKU5rzjfRy3N"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType\n",
        "from pyspark.sql.types import DoubleType, StringType, IntegerType\n",
        "\n",
        "hero_schema = (StructType()\n",
        "  .add('Id', IntegerType(), False)\n",
        "  .add('name', StringType(), True)\n",
        "  .add('Gender', StringType(), True)\n",
        "  .add('Eye color', StringType(), True)\n",
        "  .add('Race', StringType(), True)\n",
        "  .add('Hair color', StringType(), True)\n",
        "  .add('Height', DoubleType(), True)\n",
        "  .add('Publisher', StringType(), True)\n",
        "  .add('Skin color', StringType(), True)\n",
        "  .add('Alignment', StringType(), True)\n",
        "  .add('Weight', DoubleType(), True))\n",
        "\n",
        "(heros :=\n",
        " spark.read.csv('./sample_data/heroes_information.csv', header=True, schema=hero_schema, nullValue='-')\n",
        ").limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tYN8gakRy3O"
      },
      "source": [
        "## `pyspark.sql` queries are like `SQL` queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOIpIXD3Ry3P"
      },
      "source": [
        "#### Filter, group, and aggregate (categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9_8IAAVRy3P"
      },
      "outputs": [],
      "source": [
        "(heros\n",
        ".where(col('Gender') == 'Male')\n",
        ".groupby('Eye color')\n",
        ".count()\n",
        ".limit(5)\n",
        ").toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A-XJLCaRy3P"
      },
      "source": [
        "#### Group by multiple and aggregate (categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJJXHMWRDX3M"
      },
      "outputs": [],
      "source": [
        "(heros\n",
        " .groupby('Eye color', 'Gender')\n",
        " .count()\n",
        " .limit(5)\n",
        ").toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajOYRVqQRy3Q"
      },
      "source": [
        "## <font color=\"red\"> Exercise 4.2 </font>\n",
        "\n",
        "First, define a `schema` and read in `./data/super_hero_powers.csv`, then perform `pyspark.sql` queries to answer each of the following questions.\n",
        "\n",
        "1. How many heroes have both Super Strength and Super Speed?\n",
        "2. How many heroes have names that start with the word *Black*\n",
        "3. Are heroes with Agility more likely to have Stealth?\n",
        "4. What fraction of all heroes that can fly also have Super Strength?\n",
        "5. Consider heroes that have names that contain `\"girl\"`, `\"boy\"`, `\"woman\"`, or `\"man\"`.  Compute the following ratio\n",
        "\n",
        "$$\\frac{N(\\text{boy or man})}{N(\\text{girl or woman})}$$\n",
        "\n",
        "**Hint:** You will need to use some combination of `where`, `group_by`, and `count` for each part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p-RfxrTDX3M"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "ls -alG sample_data | grep hero"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/DSCI-326/DSCI326_module_6_lazy_operations/refs/heads/main/data/super_hero_powers.csv -0 ./sample_data/super_hero_powers.csv\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6DxV7pdRblsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(powers := spark.read.csv('./sample_data/super_hero_powers.csv', header=True, inferSchema=True))"
      ],
      "metadata": {
        "id": "IBbb0XSIcruv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnmzuneIRy3O"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType\n",
        "from pyspark.sql.types import BooleanType, StringType\n",
        "\n",
        "# 1.\n",
        "(powers\n",
        ".where(col('Super Strength') == 'True')\n",
        ".where(col('Super Speed') == 'True')\n",
        ".count()\n",
        ").toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ugIMUfSRy3Q"
      },
      "outputs": [],
      "source": [
        "#2.\n",
        "(powers.name.contains('Black'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.\n",
        "(powers\n",
        ".where(col('Agility') == 'True')\n",
        ".where(col('Stealth') == 'True')\n",
        ".count()\n",
        ").toPandas()"
      ],
      "metadata": {
        "id": "2TG40B4vbz_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.\n",
        "(powers\n",
        ".where(col('Flight') == 'True')\n",
        ".where(col('Super Strength') == 'True')\n",
        ".count()\n",
        ").toPandas()"
      ],
      "metadata": {
        "id": "m_zu3pzvbzxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5.\n",
        "(powers.where(col('name').contains('girl') | col('name').contains('boy') | col('name').contains('woman') | col('name').contains('man'))\n",
        ".count())"
      ],
      "metadata": {
        "id": "shI-g1xcbzhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdXTzICWRy3Q"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_00BgsBRy2z"
      },
      "source": [
        "## Creating a `Row` of data\n",
        "\n",
        "* Use the `Row` class\n",
        "* Pass data using keywords\n",
        "    * key == column name\n",
        "    * value == cell value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYA3aFPZRy2z"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "department1 = Row(id='123456', name='Computer Science')\n",
        "department1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2IUtKK2Ry20"
      },
      "source": [
        "## Unpacking a `Row` dictionary\n",
        "\n",
        "* Data is in a row dictionary\n",
        "* Unpack keywords using `**`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh10MUdoRy20",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "dept2_info = {'id':'789012', 'name':'Mechanical Engineering'}\n",
        "department2 = Row(**dept2_info)\n",
        "department2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rgaAdSMRy21"
      },
      "source": [
        "## Unpacking a list of row dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zphHlUuRy22",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "dept_info = [{'id':123456, 'name':'Computer Science'},\n",
        "             {'id':789012, 'name':'Mechanical Engineering'},\n",
        "             {'id':345678, 'name':'Theater and Drama'},\n",
        "             {'id':901234, 'name':'Indoor Recreation'}]\n",
        "\n",
        "dept_rows = [Row(**r) for r in dept_info]\n",
        "dept_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfuTevyXRy23"
      },
      "source": [
        "## Access `Row` content with column attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmfDZh-_Ry23"
      },
      "outputs": [],
      "source": [
        "[dept.id for dept in dept_rows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udaaiTSfRy24",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "[dept.name for dept in dept_rows]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqRGmJk4Ry25"
      },
      "source": [
        "## Creating a `pyspark.DataFrame`\n",
        "\n",
        "* A `DataFrame` is a collection of `Row`s\n",
        "* Create with spark.createDataFrame\n",
        "* Need to have a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h-qJ8rbRy25"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame(dept_rows)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X5xsQeCDX3N"
      },
      "source": [
        "## Creating rows from list of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu-N03ygRy3R"
      },
      "source": [
        "## Creating a Row class\n",
        "\n",
        "* Pass `Row` the columns names\n",
        "* Creates a specialized `Row` class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZO3CiYYRy3R"
      },
      "outputs": [],
      "source": [
        "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
        "Employee"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrAAWT4nRy3S"
      },
      "source": [
        "## Creating a `Employee` instance\n",
        "\n",
        "* Pass the data to `Employee` to make a row\n",
        "* Order matters ... use the same order as names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auFhqmbNRy3S"
      },
      "outputs": [],
      "source": [
        "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
        "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
        "employee1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-3Q8QpZRy3S"
      },
      "source": [
        "## Unpacking a data list\n",
        "\n",
        "* Suppose the data is in a list/tuple.\n",
        "* Use sequence unpacking with `*`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJv1aQpRRy3T"
      },
      "outputs": [],
      "source": [
        "empl2_info = ('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
        "empl2_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dRAOZ-CRy3T"
      },
      "outputs": [],
      "source": [
        "employee2 = Employee(*empl2_info)\n",
        "employee2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igGducXvRy3T"
      },
      "source": [
        "## Unpacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g64SPRukRy3U"
      },
      "outputs": [],
      "source": [
        "# Create the Employees\n",
        "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
        "employees = [('michael', 'armbrust', 'no-reply@berkeley.edu', 100000),\n",
        "             ('xiangrui', 'meng', 'no-reply@stanford.edu', 120000),\n",
        "             ('matei', None, 'no-reply@waterloo.edu', 140000),\n",
        "             (None, 'wendell', 'no-reply@berkeley.edu', 160000)]\n",
        "emp_rows = [Employee(*r) for r in employees]\n",
        "emp_rows"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Pyspark",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}